{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjn_UGpu6IB-"
   },
   "source": [
    "# Написать программу для построения обратного индекса с учетом векторного представления слов (Word2Vec, Bert). Необходимо реализовать возможность поиска по нескольким словам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L4BAS3sKvzFi",
    "outputId": "e5c0bee9-95f1-413a-beb2-7eaa7fe1280e"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2ZAW5N-ZcFVP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from transformers import BertTokenizer, BertModel, logging\n",
    "import os\n",
    "import unicodedata\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "logging.set_verbosity_error()\n",
    "warnings.filterwarnings(action='ignore', category=RuntimeWarning) # setting ignore as a parameter and further adding category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not sys.version_info >= (3, 5):\n",
    "    print('Для работы программы требуется версия Pyhton не меньше 3.5')\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V5D-d1AlwzAB",
    "outputId": "b23e62ff-6bbf-4fe1-8280-35367c7e1b51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR='data'\n",
    "TOP_TXT=3\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained('bert-base-uncased',output_hidden_states = True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edJ_S4S_Y2k3",
    "outputId": "4aed552f-8421-428b-d654-295bb6c410d9"
   },
   "outputs": [],
   "source": [
    "def file_to_tokens(filename) -> list[str]:\n",
    "    \"\"\"\n",
    "    Функция, которая для каждого документа составляет список слов (токенов). \n",
    "    Лемматизируем, токенизируем, убираем знаки препинания.\n",
    "    \"\"\"\n",
    "    text = Path(filename).read_text()\n",
    "    text = text.replace('\\n', '')\n",
    "    #text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]','',unicodedata.normalize(\"NFKC\",text))\n",
    "\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    with torch.no_grad():\n",
    "        hidden_states = model(tokens_tensor, segments_tensors)[2]\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    token_vecs_sum = [torch.sum(x[-4:], dim=0) for x in token_embeddings]\n",
    "    ret = [[tokenized_text[x], token_vecs_sum[x]] for x in range(len(tokenized_text))]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5fi9JlmoY9g2"
   },
   "outputs": [],
   "source": [
    "class ReverseIndex:\n",
    "    def __init__(self, documents) -> None:\n",
    "        \"\"\"\n",
    "        Конструктор класса.\n",
    "        documents -- список файлов.\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        self.bert_data = dict()\n",
    "        self.count_words_in_documents = dict()\n",
    "\n",
    "    def similarity(self, word1, word2) -> float:\n",
    "        \"\"\"\n",
    "        Косинусное сходство между словами.\n",
    "        word1 -- первое слово.\n",
    "        word2 -- второе слово.\n",
    "        \"\"\"\n",
    "        return 1. - cosine(word1, word2)\n",
    "\n",
    "    def find_vector(self, line) -> list:\n",
    "        \"\"\"\n",
    "        Функция, получающая вектор для строки на основе модели Национального корпуса русского языка.\n",
    "        line -- слово/строка, для которой нужно получить вектор/ы.\n",
    "        \"\"\"\n",
    "        if(type(line) != str):\n",
    "            single = False\n",
    "            line = \" \".join(line)\n",
    "        else:\n",
    "            single = True\n",
    "        marked_text = \"[CLS] \" + line + \" [SEP]\"\n",
    "        tokenized_text = tokenizer.tokenize(marked_text)\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        segments_ids = [1] * len(indexed_tokens) \n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "        with torch.no_grad():\n",
    "            hidden_states = model(tokens_tensor, segments_tensors)[2]\n",
    "        token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        token_embeddings = token_embeddings.permute(1, 0, 2)\n",
    "        token_vecs_sum = [torch.sum(x[-4:], dim=0) for x in token_embeddings]\n",
    "        return token_vecs_sum[1] if single else token_vecs_sum\n",
    "\n",
    "    def get_tokens_from_document(self, docId) -> dict():\n",
    "        \"\"\"\n",
    "        Функция, получающая список токенов из документа по его идентификатору.\n",
    "        docId -- идентификатор документа.\n",
    "        \"\"\"\n",
    "        return [x for x in self.bert_data.keys() if docId in self.bert_data[x].keys()]\n",
    "\n",
    "    def word_counter(self, docId, info) -> tuple:\n",
    "        \"\"\"\n",
    "        Получает список слов, которые наиболее часто встречаются в документе, \n",
    "        и их косинусные расстояния с искомым вектором.\n",
    "        docId -- идентификатор документа.\n",
    "        info -- словарь, где ключи -- идентификаторы документов, \n",
    "        а значения -- список из словарей с ключами \"слово_документ_позиция\" и значениями \"похожесть слова на искомое\".\n",
    "        \"\"\"\n",
    "        words = info[docId]\n",
    "        words.sort(key=lambda x: x[1])\n",
    "        best_3 = words[-3:]\n",
    "        words = [x[0] for x in best_3 if x[1] > 0.3]\n",
    "        sims = [x[1] for x in best_3 if x[1] > 0.3]\n",
    "        return words, sims, math.fsum(sims)\n",
    "\n",
    "    def multiple_words_counter(self, docId, info) -> tuple:\n",
    "        \"\"\"\n",
    "        Функция, получающая список слов и позиций в документе, которые наиболее часто встречаются в документе, \n",
    "        и их косинусные расстояния с искомым словом.\n",
    "        docId -- идентификатор документа.\n",
    "        info -- словарь, где ключи -- идентификаторы документов, \n",
    "        а значения -- список из словарей с ключами \"слово_документ_позиция\" и значениями \"похожесть слова на искомое\".\n",
    "        \"\"\"\n",
    "        words = info[docId]\n",
    "        words.sort(key=lambda x: x[1])\n",
    "        best_3 = words[-3:]\n",
    "        words = [x[0].split(\"_\")[0] for x in best_3 if x[1] > 0.3]\n",
    "        sims  = [x[1] for x in best_3 if x[1] > 0.3]\n",
    "        pos  = [x[0].split(\"_\")[1] for x in best_3 if x[1] > 0.3]\n",
    "        return words, pos, sims, math.fsum(sims)\n",
    "\n",
    "    def find_single_word(self, word) -> dict():\n",
    "        \"\"\"\n",
    "        Функция для поиска документов, содержащих одно слово.\n",
    "        word -- слово, по которому нужно осуществить поиск.\n",
    "        \"\"\"\n",
    "        results = dict()\n",
    "        docs = dict()\n",
    "        for i, document in enumerate(self.documents):\n",
    "            similarity_of_words = []\n",
    "            word_vector = self.find_vector(word)\n",
    "            for w in self.get_tokens_from_document(i):\n",
    "                for position in self.bert_data[w][i].keys():\n",
    "                    sim_rate = self.similarity(word_vector, self.bert_data[w][i][position])\n",
    "                    if i not in results.keys():  # добавляем индекс документа\n",
    "                        results[i] = []\n",
    "                    if w == word:\n",
    "                        sim_rate += 2\n",
    "                    new_w = w + \"_\" + str(position)\n",
    "                    # Храним словарь слово, документ, позиция, его похожесть с искомым\n",
    "                    results[i].append([new_w, sim_rate])  \n",
    "            words, sims, sum = self.word_counter(i, results)\n",
    "            # При таких результатах документ пропускаем\n",
    "            if(sum == 0):\n",
    "                continue\n",
    "            docs[i] = [words, sum]\n",
    "        output = dict()\n",
    "        for i in dict(sorted(docs.items(), key=lambda item: item[1][1], reverse=True)).keys():\n",
    "            output[self.documents[i]] = docs[i][0]  # позиция слова в документе\n",
    "        return output\n",
    "\n",
    "    def min_distance(self, word1, word2) -> int:\n",
    "        \"\"\"\n",
    "        Функция для определения минимального расстояния между двумя словами.\n",
    "        \"\"\"\n",
    "        # Используется лямбда-функция для вычисления расстояния между символами в соответствующих позициях в словах\n",
    "        return min(map(lambda x, y: abs(int(x) - int(y)), word1, word2)) \n",
    "\n",
    "    def find(self, *words): # -> list(str), dict\n",
    "        \"\"\"\n",
    "        Функция для нахождения слов в файлах. Если передано несколько слов, обрабатываем их внутри метода\n",
    "        для каждого слова ищем похожие, анализируем их расположение и веса.\n",
    "        Если передано одно слово - делегируем обработку в find_single_word().\n",
    "        \"\"\"\n",
    "        awords = [a.lower() for a in words]\n",
    "        # если передано одно слово, то вызываем метод find_single_word()\n",
    "        if(len(awords) == 1):\n",
    "            return self.find_single_word(awords[0])\n",
    "        results = dict()\n",
    "        docs = dict()\n",
    "        results_tmp = dict()\n",
    "        final_results = dict()\n",
    "        word_vectors = self.find_vector(words)[1:-1] # получаем векторы слов\n",
    "        for i, document in enumerate(self.documents): # итерируемся по каждому документу\n",
    "            avg_sim = [] \n",
    "            similarity_of_words = dict()\n",
    "            for idx, word_vec in enumerate(word_vectors):\n",
    "                for w in self.get_tokens_from_document(i):\n",
    "                    for position in self.bert_data[w][i].keys():\n",
    "                        # считаем меру похожести для каждого слова, которое встречается в документе\n",
    "                        sim_rate = self.similarity(word_vec, self.bert_data[w][i][position])\n",
    "                        if i not in results.keys():\n",
    "                            results[i] = []\n",
    "                        if w in awords:\n",
    "                            sim_rate += 2 # увеличиваем вес при совпадении слова с запросом\n",
    "                        new_w = w + \"_\" + str(position)\n",
    "                        results[i].append([new_w, sim_rate]) # сохраняем значение похожести для каждого слова\n",
    "                # Получаем количество совпадающих слов, сумму их весов и значение средней похожести\n",
    "                words, positions, sims, sum = self.multiple_words_counter(i, results)  \n",
    "                # Сохраняем результаты для каждого слова. Используется для вычисления среднего расстояния между словами\n",
    "                results_tmp[idx] = [words, positions, sims, sum]\n",
    "                # Добавляем значение похожести для каждого слова в список avg_sim\n",
    "                avg_sim.append(sims)\n",
    "            # При таких результатах документ пропускаем\n",
    "            if(len(results_tmp[0][0]) == 0):\n",
    "                continue;\n",
    "            # Вычисляем массив минимальных значений между парами слов\n",
    "            avg_distance = [self.min_distance(results_tmp[x][1], results_tmp[x + 1][1]) for x in range(len(results_tmp.keys()) - 1)]\n",
    "            # Получаем итоговое значение оптимальности для каждого документа\n",
    "            final_results[i] = [np.mean(np.array(list(chain.from_iterable(avg_sim)))), np.mean(np.array(avg_distance))]\n",
    "        # Возвращаем имена документов, отсортированные по оптимальности\n",
    "        ret = [self.documents[x] for x in dict(sorted(final_results.items(), key=lambda item: (item[1][0], item[1][1]), reverse=True)).keys()]\n",
    "        return ret\n",
    "    \n",
    "    def run(self) -> None:\n",
    "        \"\"\"\n",
    "        Метод запускает процесс индексации файлов.\n",
    "        \"\"\"\n",
    "        for i, document in enumerate(self.documents):\n",
    "            words_and_vectors = file_to_tokens(document)\n",
    "            self.count_words_in_documents[i] = len(words_and_vectors)\n",
    "\n",
    "            for idx, el in enumerate(words_and_vectors):\n",
    "                w = words_and_vectors[idx][0]\n",
    "                vector = words_and_vectors[idx][1]\n",
    "                if w not in self.bert_data: # добавляем слово\n",
    "                    self.bert_data[w] = dict()\n",
    "                if i not in self.bert_data[w].keys(): # добавляем индекс документа\n",
    "                    self.bert_data[w][i] = dict()\n",
    "                self.bert_data[w][i][idx] = vector # добавляем позицию слова и его вектор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0gXLbBrkhek3"
   },
   "outputs": [],
   "source": [
    "files = os.listdir('./' + DATA_DIR + '/') # Фрагменты статей англоязычной Википедии\n",
    "files[:] = [DATA_DIR + '\\\\' + x for x in files]\n",
    "X = ReverseIndex(files)\n",
    "X.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Y-FI5LO5nIe",
    "outputId": "4830e860-a730-4c5a-b97b-1ebee5a7e9a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data3\\\\insect.txt': ['trees_256', 'trees_35', 'trees_281'],\n",
       " 'data3\\\\willow.txt': ['species_61', 'species_40', 'trees_26'],\n",
       " 'data3\\\\disney.txt': ['things_66', 'time_18', '##s_113'],\n",
       " 'data3\\\\rainfall.txt': ['##ees_52', 'creating_50', 'sites_63'],\n",
       " 'data3\\\\pushkin.txt': ['he_92', '##eral_70', '##on_109'],\n",
       " 'data3\\\\wildfires.txt': ['and_101', 'acres_69', 'europe_113'],\n",
       " 'data3\\\\jordan.txt': ['something_22', '10_74', 'who_75'],\n",
       " 'data3\\\\tuttifrutti.txt': ['##t_101', 'a_327', '##boy_197']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.find('trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "raLlJQVgjvQW",
    "outputId": "be26e7d3-a4ed-43fa-96e2-35121e57073b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data3\\\\insect.txt': ['ce_204', '##hy_224', 'sy_222'],\n",
       " 'data3\\\\willow.txt': ['the_98', 'it_115', 'species_40'],\n",
       " 'data3\\\\wildfires.txt': ['##wil_89']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.find_single_word('Symphyta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DElk3_4Y3osd",
    "outputId": "f945ed59-facc-4144-8f3e-36b8b049a672"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data3\\\\disney.txt': ['time_18', '##s_113', 'ariel_33'],\n",
       " 'data3\\\\pushkin.txt': ['##eral_70', '##on_109'],\n",
       " 'data3\\\\rainfall.txt': ['the_24', 'sites_63'],\n",
       " 'data3\\\\wildfires.txt': ['europe_113'],\n",
       " 'data3\\\\tuttifrutti.txt': ['boom_409'],\n",
       " 'data3\\\\jordan.txt': ['lakers_86'],\n",
       " 'data3\\\\willow.txt': ['##most_39']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.find_single_word('Ariel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "INAuT7WS3rz-",
    "outputId": "f2052f9e-629f-4fea-f2bb-55d1ac4d2750"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data3\\\\insect.txt': ['wasp_145', 'larvae_274', 'cater_59'],\n",
       " 'data3\\\\willow.txt': ['herb_103', 'willow_100']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.find_single_word(\"caterpillar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-xzVpx63x1uJ",
    "outputId": "d22ce06c-e398-478b-b398-e8d56df6921a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data3\\\\disney.txt', 'data3\\\\pushkin.txt', 'data3\\\\willow.txt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.find('Alladdin', 'Beauty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-DuFmhfQ3yFl",
    "outputId": "6d2f07a4-07a0-41f9-fa77-089a38fb1e65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data3\\\\willow.txt',\n",
       " 'data3\\\\insect.txt',\n",
       " 'data3\\\\disney.txt',\n",
       " 'data3\\\\jordan.txt',\n",
       " 'data3\\\\pushkin.txt',\n",
       " 'data3\\\\wildfires.txt',\n",
       " 'data3\\\\tuttifrutti.txt',\n",
       " 'data3\\\\rainfall.txt']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.find('especially', 'willow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data3\\\\willow.txt',\n",
       " 'data3\\\\insect.txt',\n",
       " 'data3\\\\disney.txt',\n",
       " 'data3\\\\pushkin.txt',\n",
       " 'data3\\\\wildfires.txt',\n",
       " 'data3\\\\jordan.txt',\n",
       " 'data3\\\\tuttifrutti.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.find('willow', 'especially')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разный результат у тестов в зависимости от порядка слов. Словосочетание \"willow especially\" выглядит неграмотно и приоритетнее оказывается первое слово - willow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data3\\\\insect.txt': ['especially_284'],\n",
       " 'data3\\\\disney.txt': ['##s_113'],\n",
       " 'data3\\\\pushkin.txt': ['##on_109'],\n",
       " 'data3\\\\wildfires.txt': ['europe_113'],\n",
       " 'data3\\\\tuttifrutti.txt': ['boom_409']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.find('especially')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0PKBqLY4o_Z",
    "outputId": "1db8c2ba-840f-4ba5-9fd5-2445b012f968"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data3\\\\jordan.txt',\n",
       " 'data3\\\\disney.txt',\n",
       " 'data3\\\\rainfall.txt',\n",
       " 'data3\\\\pushkin.txt',\n",
       " 'data3\\\\tuttifrutti.txt',\n",
       " 'data3\\\\insect.txt',\n",
       " 'data3\\\\willow.txt',\n",
       " 'data3\\\\wildfires.txt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.find('over', 'the', 'lakers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data3\\\\insect.txt',\n",
       " 'data3\\\\willow.txt',\n",
       " 'data3\\\\rainfall.txt',\n",
       " 'data3\\\\pushkin.txt',\n",
       " 'data3\\\\wildfires.txt',\n",
       " 'data3\\\\disney.txt',\n",
       " 'data3\\\\jordan.txt',\n",
       " 'data3\\\\tuttifrutti.txt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.find('trees', 'shrubs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data3\\\\wildfires.txt',\n",
       " 'data3\\\\insect.txt',\n",
       " 'data3\\\\disney.txt',\n",
       " 'data3\\\\jordan.txt',\n",
       " 'data3\\\\pushkin.txt',\n",
       " 'data3\\\\tuttifrutti.txt',\n",
       " 'data3\\\\willow.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.find('have', 'burned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data3\\\\pushkin.txt']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.find('Emperor', 'court')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
